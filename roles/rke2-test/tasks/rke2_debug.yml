---

# Print the RKE2 version

- name: Print the RKE2 version
  debug:
    msg: "RKE2 version: {{ rke2_version }}"
  tags: [ 'never', 'rke2-setup' ]

# Print the RKE2 server version

- name: Print the RKE2 server version
  debug:
    msg: "RKE2 server version: {{ rke2_server_version }}"
  ignore_errors: True
  tags: [ 'never', 'rke2-setup' ]

# Print the RKE2 agent version

- name: Print the RKE2 agent version
  debug:
    msg: "RKE2 agent version: {{ rke2_agent_version }}"
  ignore_errors: True
  tags: [ 'never', 'rke2-setup' ]

# Print the RKE2 server image

- name: Print the RKE2 server image
  debug:
    msg: "RKE2 server image: {{ rke2_server_image }}"
  ignore_errors: True
  tags: [ 'never', 'rke2-setup' ]

# Print the RKE2 agent image

- name: Print the RKE2 agent image
  debug:
    msg: "RKE2 agent image: {{ rke2_agent_image }}"
  ignore_errors: True
  tags: [ 'never', 'rke2-setup' ]

# Print load_balancer_ip_internal variable

- name: Print load_balancer_ip_internal variable
  debug:
    msg: "load_balancer_ip_internal: {{ load_balancer_ip_internal }}"
  ignore_errors: True
  tags: [ 'never', 'rke2-setup' ]

# Print rke2_token variable

- name: Set path to RKE2 binaries in PATH on RKE2 server nodes
  lineinfile:
    path: ~/.bashrc
    regexp: '^# Unconditionally load the RKE2 bin directory into our path'
    line: 'export PATH=$PATH:/var/lib/rancher/rke2/bin'
    insertbefore: "# If not running interactively, don't do anything"
  when: rke2_type == 'server'
  tags: [ 'never', 'rke2-setup' ]

# Strangely the rke2 role we're using leaves the permissions on rke2.yaml wide open..

- name: On all nodes, ensure /etc/rancher/rke2/rke2.yaml is permission 600
  ansible.builtin.file:
    path: /etc/rancher/rke2/rke2.yaml
    mode: 0600
  tags: [ 'never', 'rke2-setup' ]
  ignore_errors: True

# copy the kubeconfig to the root user's home directory
# root@awx1:~# cat ~/.kube/config

- name: On RKE2 server nodes, ensure that the /root/.kube directory exists
  ansible.builtin.file:
    path: /root/.kube
    state: directory
    mode: 0700
  when: rke2_type == 'server'
  tags: [ 'never', 'rke2-setup' ]

- name: On RKE2 server nodes, copy the kubeconfig to root user's home directory
  ansible.builtin.copy:
    src: /etc/rancher/rke2/rke2.yaml
    dest: /root/.kube/config
    mode: 0600
    remote_src: true
  when: rke2_type == 'server'
  tags: [ 'never', 'rke2-setup' ]

# Worker nodes aren't being labelled strangely...

- name: Label the additional nodes as worker
  delegate_to: "{{ rke2_bootstrap_node }}"
  ansible.builtin.shell:
    cmd: kubectl label node {{ rke2_subdomain }}.{{ awx_domain.split('.')[1:] | join('.') }} node-role.kubernetes.io/worker=worker --overwrite
    executable: /bin/bash
  when: rke2_type == 'agent'
  tags: [ 'never', 'rke2-setup' ]

# ^ Why aren't these being labelled by the role?

- name: Confirm cluster is functional
  delegate_to: "{{ rke2_bootstrap_node }}"
  ansible.builtin.shell: |
    kubectl get nodes
  register: cluster_status
  run_once: True
  tags: [ 'never', 'rke2-setup' ]

- name: Display cluster status output
  ansible.builtin.debug:
    msg: "{{ cluster_status.stdout_lines }}"
  run_once: True
  tags: [ 'never', 'rke2-setup' ]
